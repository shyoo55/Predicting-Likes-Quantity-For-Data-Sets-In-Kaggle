{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web Scraping Kaggle.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akwyqZtGMARj"
      },
      "source": [
        "# **Predicting Likes Quantity For Data Sets In Kaggle**\n",
        "**By(Shaikha Bin Ateeq)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T00GXV_b12GQ"
      },
      "source": [
        "# **Overview:**\n",
        "\n",
        "Kaggle is a data science and machine learning online community. Users can use Kaggle to acquire and publish data sets, explore and build models in a web-based data-science environment, collaborate with other data scientists and machine learning experts, and compete in data science challenges.[1] However the number of likes on the dataset may be considered a significant point, as the selection of the dataset may depend on it, to develop a solution that aims to predict the number of likes on data set based on a machine learning algorithm; specifically, we will be using a linear regression model. To do so, we will use a scraped date from the Kaggle dataset in ‘computer since’ ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-AOLgNX19bO"
      },
      "source": [
        "**Import selenium:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdriJIznNapk",
        "outputId": "51f5f14d-2227-431f-c355-e22f869f6dba"
      },
      "source": [
        "pip install -U selenium"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n",
            "\u001b[K     |████████████████████████████████| 958 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure]~=1.26\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 46.0 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 32.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.0 h11-0.12.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.1.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.7 wsproto-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7BadSCINjRz",
        "outputId": "6bfc5f0c-7247-411e-8a4c-82c9aaedf350"
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# open it, go to a website, and get results\n",
        "driver = webdriver.Chrome('chromedriver',options=options)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,442 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,430 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [828 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,812 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,224 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,868 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [930 kB]\n",
            "Fetched 12.8 MB in 4s (3,460 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 94.0 MB of archives.\n",
            "After this operation, 324 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 95.0.4638.69-0ubuntu0.18.04.1 [1,135 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 95.0.4638.69-0ubuntu0.18.04.1 [83.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 95.0.4638.69-0ubuntu0.18.04.1 [4,249 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 95.0.4638.69-0ubuntu0.18.04.1 [4,986 kB]\n",
            "Fetched 94.0 MB in 5s (19.3 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_95.0.4638.69-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (4.1.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.9.2)\n",
            "Requirement already satisfied: urllib3[secure]~=1.26 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.26.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.19.0)\n",
            "Requirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.10)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.2.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.7/dist-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
            "Requirement already satisfied: pyOpenSSL>=0.14 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (21.0.0)\n",
            "Requirement already satisfied: cryptography>=1.3.4 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (36.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5UMTbxI2Dar"
      },
      "source": [
        "**Import Library:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oUSiRj_NawG",
        "outputId": "073bc0bc-3fd8-4ec8-b34b-73120b348150"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time, os\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AZRQv_w2fbb"
      },
      "source": [
        "**Web Scraping:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcdZkb2yK5GN"
      },
      "source": [
        "#initialization \n",
        "page = 2\n",
        "Data_Name=[] \n",
        "Created_By=[] \n",
        "Like=[]\n",
        "Type=[]\n",
        "License=[]\n",
        "Code=[]\n",
        "Download_GB=[]\n",
        "Views=[]\n",
        "Num_Download=[]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "while page != 14:\n",
        "\n",
        " #----------------------------------------< Requests The URL >----------------------------------------------\n",
        "      \n",
        "       URL = f\"https://www.kaggle.com/datasets?tags=12107-Computer+Science&page={page}\"\n",
        "      # print(URL)\n",
        "       page_REQ = requests.get(URL).text\n",
        "       soup = BeautifulSoup(page_REQ, 'html5lib')\n",
        "       driver.get(URL)\n",
        "       soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "       \n",
        "       \n",
        "      \n",
        "       \n",
        "\n",
        "\n",
        "#---------------------------------< Dataset Reprsent Each Container Div >-------------------------------------- \n",
        "       dataset = soup.findAll(class_=\"sc-cOifOu fDKoDk\")\n",
        "\n",
        "\n",
        "#----------------------------------------< Scrap Data Set Name >-----------------------------------------------------\n",
        "      \n",
        "      \n",
        "       for datasets in dataset :#Data_Name\n",
        "                 if datasets.find('div', class_='sc-iCoGMd sc-hHEiqL sc-laZMeE cHRTxu ccTnQh cuPfjw') is not None:\n",
        "                     name = datasets.find('div',class_='sc-iCoGMd sc-hHEiqL sc-laZMeE cHRTxu ccTnQh cuPfjw').text\n",
        "                     Data_Name.append(name)\n",
        "                    \n",
        "                 else:\n",
        "                     Data_Name.append(None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------< Scrap Created By >-----------------------------------------------\n",
        "\n",
        "       for datasets in dataset :#Created_By\n",
        "                 if datasets.find('a', class_='sc-iTVJFM sc-iBzEeX fTkvWb hLyTFD') is not None:\n",
        "                     creat = datasets.find('a',class_='sc-iTVJFM sc-iBzEeX fTkvWb hLyTFD').text\n",
        "                     Created_By.append(creat)\n",
        "                    \n",
        "                 else:\n",
        "                     Created_By.append(None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------------< Scrap Rank >-----------------------------------------------------\n",
        "\n",
        "       for datasets in dataset :#Type\n",
        "                 if datasets.find('span', class_='sc-bYwzuL sc-kLojOw bwxWTW iehvjW') is not None:\n",
        "                     ty = datasets.find('span',class_='sc-bYwzuL sc-kLojOw bwxWTW iehvjW').text #have many class \n",
        "                     Type.append(ty)\n",
        "                    \n",
        "                 else:\n",
        "                     Type.append(None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------------< Requests The URL Inside Page >-------------------------------------------------\n",
        "#--------------------------------------< Colect Each Href And Concact with https://www.kaggle.com\" To Acsees The Full Link  >---------------\n",
        "\n",
        "       for datasets in dataset:\n",
        "                 for link in datasets.find_all('a',class_=\"sc-hTRkXV enlJuL\" ,href=True):\n",
        "                   GetURL=link.get(\"href\")\n",
        "                  # print(link.get(\"href\"))\n",
        "                 \n",
        "                 Kaggle = \"https://www.kaggle.com\"\n",
        "                 DataURL=Kaggle+GetURL\n",
        "                 print(DataURL)\n",
        "                 page_REQ2 = requests.get(DataURL).text\n",
        "                 soup2 = BeautifulSoup(page_REQ2, 'html5lib')\n",
        "                 driver.get(DataURL)\n",
        "                 soup2 = BeautifulSoup(driver.page_source, 'lxml')\n",
        "                  \n",
        "\n",
        "\n",
        "#------------------------------------------------< Scrap Download Size  >-----------------------------------------------------\n",
        "                 \n",
        "                   \n",
        "                 if soup2.find('p',class_='sc-fKgJPI sc-bCwfaz sc-iJvUvI bSYwtb hzzSzX PIvAF') is not None:\n",
        "                   dogb = soup2.find('p',class_='sc-fKgJPI sc-bCwfaz sc-iJvUvI bSYwtb hzzSzX PIvAF').text \n",
        "                   Download_GB.append(dogb)\n",
        "                 else:\n",
        "                   Download_GB.append(0)\n",
        "                  \n",
        "                  \n",
        "\n",
        "#------------------------------------------------< Scrap License >-----------------------------------------------------\n",
        "\n",
        "                  \n",
        "                 if soup2.find('a',class_='QuickInfo_A--43d8pb bAnHXR') is not None:\n",
        "                   lin = soup2.find('a',class_='QuickInfo_A--43d8pb bAnHXR').text \n",
        "                   License.append(lin)\n",
        "                    \n",
        "                 else:\n",
        "                   License.append(None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------------< Scrap Number Of Notebook >-----------------------------------------------------\n",
        "\n",
        "                 if soup2.find('a',id='pageheader-nav-item--code') is not None:\n",
        "                   co = soup2.find('span',class_='pageheader__link-count').find('span').text \n",
        "                   Code.append(co)\n",
        "                    \n",
        "                 else:\n",
        "                   Code.append(0)\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------------< Scrap Number Of Like >----------------------------------------------------------\n",
        "\n",
        "                 if soup2.find('span',class_='vote-button__vote-count') is not None:\n",
        "                   lik = soup2.find('span',class_='vote-button__vote-count').text \n",
        "                   Like.append(lik)\n",
        "                    \n",
        "                 else:\n",
        "                   Like.append(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " #------------------------------------------------< Scrap Number Of Views >----------------------------------------------------------\n",
        "\n",
        "                 if soup2.find('li',class_='horizontal-list-item horizontal-list-item--bullet horizontal-list-item--default') is not None:\n",
        "                   vi = soup2.find('li',class_='horizontal-list-item horizontal-list-item--bullet horizontal-list-item--default').find('span').text \n",
        "                   Views.append(vi)\n",
        "                    \n",
        "                 else:\n",
        "                   Views.append(0)\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       page = page + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aevzHttd-m2F",
        "outputId": "f2abbed4-722a-4418-aa6e-11caf9ca90d3"
      },
      "source": [
        " #-----< Check From All Len Before Incsert It Into Data Fream   >------\n",
        " \n",
        "print(len(Data_Name))\n",
        "print(len(Created_By))\n",
        "print(len(Type))\n",
        "print(len(Like))\n",
        "print(len(Download_GB))\n",
        "print(len(Code))\n",
        "print(len(Views))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "A_L386Xi6Jjh",
        "outputId": "8d9af974-473d-46fa-b5ab-dbfed0168a8b"
      },
      "source": [
        " #-----< Incsert All Array Into Data Fream   >------\n",
        " \n",
        "df=pd.DataFrame({'DataSet Name':Data_Name,'Created By':Created_By,'Rank':Type,'Like':Like,'Download Size':Download_GB,\n",
        "                 'Notebook':Code,'View':Views })\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DataSet Name</th>\n",
              "      <th>Created By</th>\n",
              "      <th>Rank</th>\n",
              "      <th>Like</th>\n",
              "      <th>Download Size</th>\n",
              "      <th>Notebook</th>\n",
              "      <th>View</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018 Kaggle Machine Learning &amp; Data Science Su...</td>\n",
              "      <td>Not Definde</td>\n",
              "      <td>Gold</td>\n",
              "      <td>987</td>\n",
              "      <td>42.56 MB</td>\n",
              "      <td>472</td>\n",
              "      <td>388,207 views</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Keras Pretrained models</td>\n",
              "      <td>beluga</td>\n",
              "      <td>Gold</td>\n",
              "      <td>400</td>\n",
              "      <td>1.07 GB</td>\n",
              "      <td>547</td>\n",
              "      <td>61,726 views</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IPL _Data_Set</td>\n",
              "      <td>Ramji</td>\n",
              "      <td>Gold</td>\n",
              "      <td>250</td>\n",
              "      <td>18.43 MB</td>\n",
              "      <td>1</td>\n",
              "      <td>63,513 views</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017 Kaggle Machine Learning &amp; Data Science Su...</td>\n",
              "      <td>Not Definde</td>\n",
              "      <td>Gold</td>\n",
              "      <td>838</td>\n",
              "      <td>29.23 MB</td>\n",
              "      <td>427</td>\n",
              "      <td>203,837 views</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NIH Chest X-rays</td>\n",
              "      <td>Not Definde</td>\n",
              "      <td>Gold</td>\n",
              "      <td>861</td>\n",
              "      <td>45.08 GB</td>\n",
              "      <td>1</td>\n",
              "      <td>371,760 views</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>OECD Productivity Data</td>\n",
              "      <td>Jessica Yung</td>\n",
              "      <td>Bronze</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7,063 views</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>Subreddit Interactions for 25,000 Users</td>\n",
              "      <td>colemaclean</td>\n",
              "      <td>Bronze</td>\n",
              "      <td>23</td>\n",
              "      <td>507.59 MB</td>\n",
              "      <td>3</td>\n",
              "      <td>11,253 views</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>New York City Taxi Trip - Distance Matrix</td>\n",
              "      <td>deb</td>\n",
              "      <td>Bronze</td>\n",
              "      <td>15</td>\n",
              "      <td>4.78 MB</td>\n",
              "      <td>9</td>\n",
              "      <td>9,622 views</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>Pre-trained Word Vectors for Spanish</td>\n",
              "      <td>Rachael Tatman</td>\n",
              "      <td>Bronze</td>\n",
              "      <td>72</td>\n",
              "      <td>2.87 GB</td>\n",
              "      <td>5</td>\n",
              "      <td>20,738 views</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>Video Object Tracking</td>\n",
              "      <td>K Scott Mader</td>\n",
              "      <td>Bronze</td>\n",
              "      <td>39</td>\n",
              "      <td>3.6 GB</td>\n",
              "      <td>2</td>\n",
              "      <td>15,874 views</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          DataSet Name  ...           View\n",
              "0    2018 Kaggle Machine Learning & Data Science Su...  ...  388,207 views\n",
              "1                              Keras Pretrained models  ...   61,726 views\n",
              "2                                        IPL _Data_Set  ...   63,513 views\n",
              "3    2017 Kaggle Machine Learning & Data Science Su...  ...  203,837 views\n",
              "4                                     NIH Chest X-rays  ...  371,760 views\n",
              "..                                                 ...  ...            ...\n",
              "195                             OECD Productivity Data  ...    7,063 views\n",
              "196            Subreddit Interactions for 25,000 Users  ...   11,253 views\n",
              "197          New York City Taxi Trip - Distance Matrix  ...    9,622 views\n",
              "198               Pre-trained Word Vectors for Spanish  ...   20,738 views\n",
              "199                              Video Object Tracking  ...   15,874 views\n",
              "\n",
              "[200 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgv_wqGn6JsK"
      },
      "source": [
        " #-----< Save The Data Fream Into CSV File  >------\n",
        "df.to_csv('data1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9JB-whsLbWQ"
      },
      "source": [
        "Since the Run will take time every time , i will save the data into CSV file and complite all work in the next Jybter < Kaggle_Linear Regression.ipynb>"
      ]
    }
  ]
}